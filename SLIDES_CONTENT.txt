================================================================================
SLIDE-BY-SLIDE CONTENT FOR COPY-PASTE
Copy this text directly into your PowerPoint slides
================================================================================

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 1: TITLE SLIDE
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Emotion Classification in Twitter Data
Using Transformer-Based Models

A Comparative Study with Model Compression

Adi Oizerovich & Roei Michael
Hebrew University of Jerusalem
NLP Course - Part B

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 2: PROJECT OVERVIEW
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Project Goal

TASK: Classify Twitter posts into 6 emotions
  • 0: Sadness
  • 1: Joy
  • 2: Love
  • 3: Anger
  • 4: Fear
  • 5: Surprise

DATASET: 18,000 English tweets
  • Training: 16,000 samples
  • Validation: 2,000 samples

CHALLENGE: Class imbalance
  • Most common: Joy (33.5%)
  • Least common: Surprise (3.6%)
  • 9× difference!

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 3: CLASS IMBALANCE
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Dataset Challenge: Class Imbalance

CLASS DISTRIBUTION:
Emotion      Count    Percentage
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Joy          5,362    33.5%
Sadness      4,666    29.2%
Anger        2,159    13.5%
Fear         1,937    12.1%
Love         1,304     8.2%
Surprise       572     3.6%

OUR SOLUTION:
✓ Weighted Cross-Entropy Loss
✓ Higher weights for minority classes
   → Surprise: 4.66×
   → Love: 2.04×
✓ F1 Macro as primary metric
   (treats all classes equally)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 4: MODELS COMPARED
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Three Transformer Models Tested

Model        Parameters    Size      Key Feature
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
BERT         109M          418 MB    Bidirectional encoder
RoBERTa      125M          476 MB    Optimized BERT
ELECTRA      109M          418 MB    Discriminator-based

ALL MODELS:
• Pre-trained on large text corpora
• Fine-tuned on Twitter emotion dataset
• 3 epochs training
• AdamW optimizer (LR: 2e-5)
• Batch size: 32

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 5: MODEL PERFORMANCE
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Results: All Models Exceed 93% Accuracy

Model        Accuracy    F1 Macro    F1 Weighted    Training Time
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
BERT         93.4%       91.08%      93.50%         85 min
RoBERTa ⭐   93.4%       91.32%      93.52%         90 min
ELECTRA      93.1%       90.82%      93.21%         84 min

WINNER: RoBERTa
• Highest F1 Macro score
• Best for imbalanced data

[IMAGE: 01_model_performance_comparison.png - RIGHT SIDE]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 6: PER-CLASS PERFORMANCE
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Performance Breakdown by Emotion

RoBERTa F1 SCORES PER CLASS:
Emotion      F1 Score    Comment
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Sadness      96.2%       Excellent
Joy          95.1%       Excellent
Anger        93.5%       Very good
Fear         88.9%       Good
Love         88.1%       Good (8% of data)
Surprise     86.2%       Good (3.6% of data!)

KEY FINDING:
Even minority classes achieve >86% F1!

[IMAGE: 02_per_class_f1_comparison.png - RIGHT SIDE]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 7: CONFUSION MATRIX
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

RoBERTa Confusion Matrix

[IMAGE: roberta_confusion_matrix.png - FULL SLIDE, LARGE]

Strong diagonal = accurate predictions
Off-diagonal = confusion between similar emotions

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 8: TRAINING RESOURCES
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Computational Efficiency

TRAINING RESOURCES:
• All models: ~85-90 minutes on GPU
• Similar memory requirements
• Batch size: 32
• Max sequence length: 128 tokens

MODEL SIZES:
• BERT/ELECTRA: ~418 MB
• RoBERTa: ~476 MB (13% larger)

TAKEAWAY:
BERT offers best size/performance tradeoff
if storage is critical

[IMAGE: 03_training_resources_comparison.png - RIGHT SIDE]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 9: COMPRESSION METHODS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Model Compression: Making Models Smaller

GOAL: Deploy on mobile/edge devices

TWO METHODS TESTED:

1. QUANTIZATION (8-bit)
   • Convert weights: 32-bit → 8-bit integers
   • Size: 476 MB → 231 MB (2.06× smaller)
   • Accuracy: 93.4% → 92.9% (0.5% drop)
   • Verdict: ✅ Excellent tradeoff!

2. PRUNING
   • Remove small-magnitude weights
   • 30% Pruning: 90.2% accuracy (3.2% drop)
     → Acceptable
   • 50% Pruning: 40.2% accuracy
     → ❌ Model collapse!

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 10: COMPRESSION RESULTS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Compression Comparison

Method           Size      Accuracy   F1 Macro   Compression
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Original         476 MB    93.4%      91.3%      1.0×
Quantized ⭐     231 MB    92.9%      90.7%      2.06×
Pruned (30%)     476 MB    90.2%      86.5%      1.0×
Pruned (50%)     476 MB    40.2%      19.4%      1.0×

RECOMMENDATION:
Quantization for deployment
→ 2× smaller, minimal accuracy loss

[IMAGE: 05_compression_tradeoff.png - LARGE, 70%]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 11: SUMMARY TABLE
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Complete Results Summary

[IMAGE: 04_summary_table.png - FULL SLIDE]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 12: TECHNICAL IMPLEMENTATION
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Implementation Details

KEY TECHNOLOGIES:
• Framework: PyTorch + Hugging Face Transformers
• Hardware: NVIDIA GPU (CUDA enabled)

TRAINING STRATEGY:
• Weighted loss for class imbalance
• Linear warmup + learning rate decay
• Gradient clipping (max_norm=1.0)
• Best model selection via F1 Macro

CODE QUALITY:
• Modular architecture (utils: data, metrics, training)
• Reproducible (fixed random seed)
• Well-documented and tested
• Clean, production-ready code

GitHub: [Your Repository Link]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 13: KEY CONTRIBUTIONS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

What We Delivered

✅ THREE TRANSFORMER MODELS trained and compared
   All exceed 80% requirement (achieved 93%!)

✅ HANDLED CLASS IMBALANCE effectively
   Weighted loss + F1 Macro evaluation
   Minority classes achieve >86% F1

✅ TWO COMPRESSION TECHNIQUES tested
   Quantization: 2× smaller, <1% accuracy loss
   Pruning: Showed limits (50% causes collapse)

✅ PRODUCTION-READY INFERENCE API
   Clean interface: run_inference(weights, csv)
   Returns predictions as list
   Saves results to CSV

✅ COMPREHENSIVE DOCUMENTATION
   Testing guide, API docs, README

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 14: KEY FINDINGS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Main Takeaways

1. MODEL SELECTION
   • RoBERTa wins for imbalanced data (highest F1 Macro)
   • BERT close second (smaller, similar accuracy)
   • All transformers performed excellently (93%+)

2. CLASS IMBALANCE
   • Weighted loss critical for minority classes
   • Even 3.6% class (Surprise) achieved 86% F1
   • F1 Macro better metric than accuracy

3. COMPRESSION
   • Quantization production-ready (2× smaller, <1% loss)
   • Pruning >50% causes model failure
   • Future: knowledge distillation

4. REAL-WORLD IMPACT
   • Social media sentiment analysis
   • Mental health monitoring
   • Customer feedback classification

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 15: FUTURE WORK
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Potential Improvements

MODEL ENHANCEMENTS:
• Test larger models (BERT-Large, RoBERTa-Large)
• Knowledge distillation (teacher-student)
• Ensemble methods

DATA IMPROVEMENTS:
• More training data for minority classes
• Data augmentation (back-translation, paraphrasing)
• Better emoji and emoticon handling

DEPLOYMENT:
• Deploy quantized model to mobile app
• Real-time Twitter stream classification
• Multi-language support

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 16: CONCLUSION
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Summary

PROBLEM:
Classify Twitter emotions with imbalanced data

SOLUTION:
Fine-tuned transformers with weighted loss

RESULTS:
• 93.4% accuracy (RoBERTa)
• 91.3% F1 Macro
• Works well even for minority classes (3.6% of data)

COMPRESSION:
• 2× size reduction with minimal accuracy loss
• Quantization recommended for deployment

DELIVERABLES:
• 3 trained models
• Production-ready inference API
• Comprehensive analysis and documentation

STATUS: ✅ Ready for production

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SLIDE 17: THANK YOU
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Thank You!
Questions?

Adi Oizerovich & Roei Michael

Contact:
adi.oizerovich@gmail.com
roeym111@gmail.com

GitHub: [Your Repository Link]

Hebrew University of Jerusalem

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
END OF SLIDES
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
